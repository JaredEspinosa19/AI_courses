{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3 - Recurrent Word2Vectors - Pokémon Legends: Arceus\n",
    "\n",
    "## Alumno: Espinosa Cruz Emilio Jared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "A lo largo del curso se ha podido conocer y trabajar con diferentes tipos de redes neuronales: convolucionales, residuales, y ahora, un nuevo tipo de redes: las recurrentes.\n",
    "\n",
    "Las redes recurrentes son un tipo de red neuronal artificial que se utilizan para procesar datos secuenciales. Son particularmente adecuadas para tareas como el reconocimiento de voz, la traducción automática y la generación de texto.\n",
    "\n",
    "Para poder entender un poco más este concepto, además de explorar de nuevo el tema de aprendizaje no supervisado, se implementará un modelo CBOW desde cero.\n",
    "\n",
    "El modelo CBOW (o bolsa de palabras) es un modelo de red neuronal que permite crear representaciones distribuidas de palabras que capturan las relaciones semánticas y sintácticas entre palabras.\n",
    "\n",
    "La implementación de este modelo se ajusta a uno de los objetivos de la práctica, ya que este crea diferentes agrupaciones con base al conjunto de datos entrenados. Por lo tanto, podremos ver qué tipo de agrupaciones crea el modelo a lo largo del modelo.\n",
    "\n",
    "Se dividirá en dos partes: la primera parte será para el preprocesamiento del conjunto de datos, las cuales implicarán aplicar técnicas de limpieza de datos para lenguaje natural, como eliminación de stop words, tokenización y lematización. La segunda parte será la implementación del modelo CBOW, donde se creará el vocabulario completo a partir de los datos, su respectiva tokenización y la implementación y entrenamiento del modelo,\n",
    " para después ver los resultados a partir de una proyección de palabras hechas por este."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descritas las tareas, se empezará con la parte del preprocesamiento del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset a emplear consiste en una colección de comentarios de diversos idiomas acerca del juego 'Pokémon Legends: Arceus'.\n",
    "\n",
    "Como primera parte ser cargará el dataset que ese encuentra en formato CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('./arceus_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargado el conjunto de datos, procedemos a explorar las primeras 5 tuplas del dataset para conocer como se encuentra conformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>helpful</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>number_of_ratings</th>\n",
       "      <th>average_user_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A fun reinvention of the Pokémon formula, hope...</td>\n",
       "      <td>Jan 29, 2022</td>\n",
       "      <td>10</td>\n",
       "      <td>49 of 62 users found this helpful</td>\n",
       "      <td>2 Reviews</td>\n",
       "      <td>2 Ratings</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fun, challenging (at times), Pokemon game wi...</td>\n",
       "      <td>Jan 30, 2022</td>\n",
       "      <td>10</td>\n",
       "      <td>52 of 72 users found this helpful</td>\n",
       "      <td>60 Reviews</td>\n",
       "      <td>111 Ratings</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El juego más infravalorado de la saga y posibl...</td>\n",
       "      <td>Jan 29, 2022</td>\n",
       "      <td>10</td>\n",
       "      <td>48 of 62 users found this helpful</td>\n",
       "      <td>8 Reviews</td>\n",
       "      <td>12 Ratings</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best pokemon game of all time! New pokemon, ne...</td>\n",
       "      <td>Jan 30, 2022</td>\n",
       "      <td>10</td>\n",
       "      <td>28 of 35 users found this helpful</td>\n",
       "      <td>1 Review</td>\n",
       "      <td>4 Ratings</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One outstanding experience, that is certain. I...</td>\n",
       "      <td>Feb 3, 2022</td>\n",
       "      <td>10</td>\n",
       "      <td>4 of 8 users found this helpful</td>\n",
       "      <td>1 Review</td>\n",
       "      <td>14 Ratings</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review          date  score  \\\n",
       "0  A fun reinvention of the Pokémon formula, hope...  Jan 29, 2022     10   \n",
       "1  A fun, challenging (at times), Pokemon game wi...  Jan 30, 2022     10   \n",
       "2  El juego más infravalorado de la saga y posibl...  Jan 29, 2022     10   \n",
       "3  Best pokemon game of all time! New pokemon, ne...  Jan 30, 2022     10   \n",
       "4  One outstanding experience, that is certain. I...   Feb 3, 2022     10   \n",
       "\n",
       "                             helpful number_of_reviews number_of_ratings  \\\n",
       "0  49 of 62 users found this helpful         2 Reviews         2 Ratings   \n",
       "1  52 of 72 users found this helpful        60 Reviews       111 Ratings   \n",
       "2  48 of 62 users found this helpful         8 Reviews        12 Ratings   \n",
       "3  28 of 35 users found this helpful          1 Review         4 Ratings   \n",
       "4    4 of 8 users found this helpful          1 Review        14 Ratings   \n",
       "\n",
       "   average_user_score  \n",
       "0                 0.0  \n",
       "1                 5.8  \n",
       "2                 7.2  \n",
       "3                 5.0  \n",
       "4                 9.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorado el dataset, procedemos a extraer la información que no interesa, las opiniones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = dataframe['review'].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificar el idioma de la review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, el dataset está conformado por diferentes idiomas, por lo tanto, se iterará cada opinión para saber en que idioma está escrito antes de preprocesarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "languajes = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "languajes_review = []\n",
    "for review in reviews:\n",
    "  languaje = detect(review)\n",
    "  languajes.add(languaje)\n",
    "  languajes_review.append(languaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ca',\n",
       " 'de',\n",
       " 'en',\n",
       " 'es',\n",
       " 'fr',\n",
       " 'id',\n",
       " 'it',\n",
       " 'ja',\n",
       " 'ko',\n",
       " 'no',\n",
       " 'pt',\n",
       " 'ru',\n",
       " 'tr',\n",
       " 'zh-cn'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenguajes detectados\n",
    "\n",
    "Para analizar en que idioma está escrita la opinión se usó la librería *langdetect*, la cual detectó las siguientes librerías.\n",
    "\n",
    "- 'ca': Catalán\n",
    "- 'de': Alemán\n",
    "- 'en': Inglés\n",
    "- 'es': Español\n",
    "- 'fr': Francés\n",
    "- 'id': Indonesio\n",
    "- 'it': Italiano\n",
    "- 'ja': Japonés\n",
    "- 'ko': Coreano\n",
    "- 'no': Noruego\n",
    "- 'pt': Portugués\n",
    "- 'ru': Ruso\n",
    "- 'tr': Turco\n",
    "- 'zh-cn': Chino Simplificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descubiertos los idiomas se procederá a procesar las opiones usando la librería *Spacy*. Sin embargo, no se realizará el procesamiento de todas las opiniones, ya que esta librería no tiene soporte para todos los idiomas detectados anteriormente. Por lo tanto, las opiniones antes mencionadas se quedarán de esa manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean listas para poder guardar cada opinión respecto a su idioma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_catalan = []\n",
    "rev_aleman = []\n",
    "rev_ingles = []\n",
    "rev_español = []\n",
    "rev_frances = []\n",
    "rev_italiano = []\n",
    "rev_japones = []\n",
    "rev_coreano = []\n",
    "rev_noruego = []\n",
    "rev_portugues = []\n",
    "rev_ruso = []\n",
    "rev_chino = []\n",
    "rev_general = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, languaje in zip(reviews, languajes_review):\n",
    "  if languaje == 'ca':\n",
    "    rev_catalan.append(review)\n",
    "  elif languaje == 'de':\n",
    "    rev_aleman.append(review)\n",
    "  elif languaje == 'en':\n",
    "    rev_ingles.append(review)\n",
    "  elif languaje == 'es':\n",
    "    rev_español.append(review)\n",
    "  elif languaje == 'fr':\n",
    "    rev_frances.append(review)\n",
    "  elif languaje == 'it':\n",
    "    rev_italiano.append(review)\n",
    "  elif languaje == 'ja':\n",
    "    rev_japones.append(review)\n",
    "  elif languaje == 'ko':\n",
    "    rev_coreano.append(review)\n",
    "  elif languaje == 'no':\n",
    "    rev_noruego.append(review)\n",
    "  elif languaje == 'pt':\n",
    "    rev_portugues.append(review)\n",
    "  elif languaje == 'ru':\n",
    "    rev_ruso.append(review)\n",
    "  elif languaje == 'zh-cn':\n",
    "    rev_chino.append(review)\n",
    "  else:\n",
    "    rev_general.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar preprocesamiento para cada una de las reviews dependiendo del idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un diccionario con distintas reglas que se aplicarán para limipiar cierto contenido de la review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dict = {\n",
    "  'user': r'\\B@\\w+',\n",
    "  'hashtag': r'\\B#\\w+',\n",
    "  'links': r'\\b(?:https?|ftp):\\/\\/\\S+',\n",
    "  # 'especialWords': r'\\b[A-Z]{2,3}\\b|&amp;|(?<=\\s)(via|Via)(?=\\s)',\n",
    "  'notCharacter': r'[“”\",.¿?¡!#$%/():&;^-]|\\b\\'(?!\\w)|(?<!\\w)\\'\\b|…',\n",
    "  'spaces': r'\\s{1,}',\n",
    "  'start': r'^\\s+',\n",
    "  'consonants': r'\\b[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanReview(review):\n",
    "  aux = re.sub(clean_dict['user'] , '', review)\n",
    "  aux = re.sub(clean_dict['hashtag'] , '', aux)\n",
    "  aux = re.sub(clean_dict['links'], '', aux)\n",
    "  # aux = re.sub(twitter_dict['especialWords'], ' ', aux)\n",
    "  aux = re.sub(clean_dict['notCharacter'], ' ', aux)\n",
    "  aux = re.sub(clean_dict['consonants'], ' ', aux)\n",
    "  aux = re.sub(clean_dict['spaces'], ' ', aux)\n",
    "  aux = re.sub(clean_dict['start'], '', aux)\n",
    "  return aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definido el diccionario y la función que realizará la acción, se procede a procesar las opiones, donde primero se aplican las reglas del diccionario y después pasan al su respectivo proceso de tokenización y lematización dependiento el idioma. Importante aclarar que, no todas las reviews pasarán por el proceso de limpieza con el diccionario, ya que, muchos manejan un sistemas de escritura diferente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ca_core_news_sm\")\n",
    "\n",
    "clean_catalan = []\n",
    "for review in rev_catalan:\n",
    "  review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_catalan.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alemán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "clean_aleman = []\n",
    "for review in rev_aleman:\n",
    "  review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_aleman.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "clean_ingles = []\n",
    "for review in rev_ingles:\n",
    "  review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_ingles.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "clean_espanol = []\n",
    "for review in rev_español:\n",
    "  review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_espanol.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Francés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "clean_frances = []\n",
    "for review in rev_frances:\n",
    "  review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_frances.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "clean_italiano = []\n",
    "for review in rev_italiano:\n",
    "  review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_italiano.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japonés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ja_core_news_sm\")\n",
    "\n",
    "clean_japones = []\n",
    "for review in rev_japones:\n",
    "  # review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_japones.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ja_core_news_sm\")\n",
    "\n",
    "clean_coreano = []\n",
    "for review in rev_coreano:\n",
    "  # review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_coreano.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noruego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nb_core_news_sm\")\n",
    "\n",
    "clean_noruego = []\n",
    "for review in rev_noruego:\n",
    "  # review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_noruego.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ruso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "clean_ruso = []\n",
    "for review in rev_ruso:\n",
    "  # review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_ruso.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "clean_chino = []\n",
    "for review in rev_chino:\n",
    "  # review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_chino.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portugues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "clean_portuges = []\n",
    "for review in rev_portugues:\n",
    "  # review = cleanReview(review)\n",
    "  doc = nlp(review)\n",
    "  aux = ''\n",
    "  for token in doc:\n",
    "    aux = aux + token.lemma_ + ' '\n",
    "    # print(token.lemma_)\n",
    "  clean_portuges.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear dataframe final para guardar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalizadas todas las tareas, se procede a crear un dataframe con todas las opiniones finales, el cual se guardará en un archivo CSV para la siguiente parte de la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({\n",
    "  'reviews': clean_aleman + clean_catalan + clean_chino + clean_coreano + clean_espanol + clean_frances + rev_general + clean_ingles + clean_italiano + clean_japones + clean_noruego + clean_portuges + clean_ruso,\n",
    "\n",
    "})\n",
    "final_df.to_csv('dataset_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente parte se realizarán las tareas restantes, como la creación del modelo y su entrenamiento, al igual que su visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic_redes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
